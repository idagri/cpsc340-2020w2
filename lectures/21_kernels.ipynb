{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 21: kernel methods\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=mba4xweShwI&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=21)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sweet Baby! by Mere Notilde\n",
    "2. THE BADDEST by K/DA\n",
    "3. 911 by Lady Gaga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a4 due tonight\n",
    "- I updated the schedule, Friday's lecture will be on stochastic gradient descent\n",
    "- Some of the toughest parts of the class are happening now (softmax, kernels, MLE/MAP, PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- linear SVM minutiae\n",
    "- n-by-n normal equations\n",
    "- feature transformation demo\n",
    "- multi-dimensional polynomial basis\n",
    "- kernel trick\n",
    "- linear regression vs. kernel regression\n",
    "- Gaussian RBF kernel\n",
    "- kernel trick applications\n",
    "- RBF SVM hyperparameters demo\n",
    "- RBF SVM minutiae\n",
    "- summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/L21/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2016W2 final\n",
    "\n",
    "Name an advantage and a disadvantage of using an SVM with the RBF kernel instead of a linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:**\n",
    "\n",
    "- Advantage of RBF (vs. linear): fit complex, nonlinear decision boundaries\n",
    "- Disadvantage of RBF (vs. linear): slow, non-parametric - need to store (part of) dataset, prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018W1 final\n",
    "\n",
    "![](img/L21/matching2_top.png)\n",
    "\n",
    "![](img/L21/matching2_bottom.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers:**\n",
    "\n",
    "- i) KNN regression \n",
    "- ii) OLS\n",
    "- iii) polynomial, because it doesn't go to zero away from the data\n",
    "- iv) RBF with smaller width\n",
    "- v) ridge regression (OLS + L2)\n",
    "- vi) RBF with larger width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo of the \"other\" normal equations and the kernel trick\n",
    "\n",
    "Consider the randomly generated dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 8, 9],\n",
       "       [5, 0, 0],\n",
       "       [1, 7, 6],\n",
       "       [9, 2, 4],\n",
       "       [5, 2, 4],\n",
       "       [2, 4, 7],\n",
       "       [7, 9, 1],\n",
       "       [7, 0, 6],\n",
       "       [9, 9, 7],\n",
       "       [6, 9, 1],\n",
       "       [0, 1, 8],\n",
       "       [8, 3, 9],\n",
       "       [8, 7, 3],\n",
       "       [6, 5, 1],\n",
       "       [9, 3, 4],\n",
       "       [8, 1, 4],\n",
       "       [0, 3, 9],\n",
       "       [2, 0, 4],\n",
       "       [9, 2, 7],\n",
       "       [7, 9, 8]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 20\n",
    "d = 3\n",
    "p = 2 \n",
    "λ = 1\n",
    "np.random.seed(1)\n",
    "X = np.random.randint(0,10,size=(n,d))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4, -2,  2,  2, -1,  0,  4, -2,  1,  3, -5, -3,  2,  2,  4,  2,\n",
       "       -2, -5,  3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.random.randint(-5, 5, n)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to use a degree 2 polynomial basis to fit this dataset, using L2 regularization with $\\lambda=1$. \n",
    "\n",
    "We then want to make a prediction on the test vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.array([[-1, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code this up 3 ways:\n",
    "\n",
    "1. Using the \"regular\" normal equations\n",
    "2. Using the \"other\" normal equations without the kernel trick\n",
    "3. Using the \"other\" normal equations with the kernel trick\n",
    "\n",
    "and show that you get the same answers all 3 ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=p)\n",
    "Z = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running time of the above step: \n",
    "\n",
    "- We have $O(d^p)$ features we're making. Let's call this $O(k)$ for now to keep it general and we can plug $k=d^p$ back in at the end.\n",
    "- We need to transform each of the $n$ training examples. \n",
    "- For each training example, for each new feature, we potentially need to look at all the old features.\n",
    "- $O(ndk)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.325003  ,  1.62053602, -1.77771719,  0.1154218 , -0.02284804,\n",
       "       -0.0384322 , -0.22760804,  0.13310881,  0.11101713,  0.04284918])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.linalg.solve(Z.T@Z + λ*np.eye(poly.n_output_features_), Z.T@y)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running time of the above step:\n",
    "\n",
    "- Multiply $Z^TZ$, meaning an $k \\times n$ multiplied by $n \\times k$ which is $O(nk^2)$\n",
    "- Then the solving is $O(k^3)$ so we have $O(nk^2 + k^3)$ which we've seen in an earlier lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58250803])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ztest = poly.transform(Xtest)\n",
    "Ztest@w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running time of the above step:\n",
    "\n",
    "- $O(k)$\n",
    "- So the total is $O(ndk+nk^2+k^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.325003  ,  1.62053602, -1.77771719,  0.1154218 , -0.02284804,\n",
       "       -0.0384322 , -0.22760804,  0.13310881,  0.11101713,  0.04284918])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_other = Z.T @ np.linalg.solve(Z@Z.T + λ*np.eye(n), y)\n",
    "w_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running time of the above step:\n",
    "\n",
    "- Multiply $ZZ^T$, meaning an $n \\times k$ multiplied by $k \\times n$ which is $O(n^2k)$\n",
    "- Then the solving is $O(n^3)$ so we have $O(n^2k + n^3)$ which we've seen in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we get the same $w$ so the prediction will also be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58250803])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ztest = poly.transform(Xtest)\n",
    "Ztest@w_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 3\n",
    "\n",
    "$K(a, b) = (1+a^Tb)^p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = (1 + np.sum(X[None] * X[:,None], axis=-1))**p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $K$ is $n \\times n$ so it costs $O(n^2d)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.linalg.solve(K + λ*np.eye(n), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Costs $n^3$ to solve the $n \\times n$ system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ktest = (1 + np.sum(X[None] * Xtest[:,None], axis=-1))**p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ktest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.8150272])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ktest @ u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get the same prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total cost $O(n^2d + n^3)$ which is independent of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of running times:\n",
    "\n",
    "- Regular approach $O(ndk+nk^2+k^3)$ \n",
    "- Other normal equations $O(ndk + n^2k + n^3)$\n",
    "- Kernel trick $O(n^2d + n^3)$\n",
    "  - Assuming kernel function costs $O(d)$ per pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelRidge(degree=2, gamma=1, kernel='polynomial')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "kr = KernelRidge(kernel=\"polynomial\", degree=2, alpha=1, coef0=1, gamma=1)\n",
    "kr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.31338522, -2.39827335, -1.45403079,  0.47762859,  1.67957059,\n",
       "       -0.31214823, -0.83869815,  2.29852489, -0.65777287,  0.9480587 ,\n",
       "        1.36667025, -1.13043066, -2.67360463,  1.85310435,  1.52846018,\n",
       "        1.39445037, -0.03268335, -3.24799773, -2.71651028,  2.73737419])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr.dual_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.8150272])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise_kernels(X, X, metric=\"polynomial\", degree=2, coef0=1, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "What is the **space complexity** of a trained model for each of the above 3 approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:**\n",
    "\n",
    "- Approach 1: $O(k)$, just storing $w$\n",
    "- Approach 2: $O(k)$, same as above\n",
    "- Approach 3: $O(nd)$, need to store the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "What is the **prediction time complexity** (for one example) for each of the above 3 approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:**\n",
    "\n",
    "- Approach 1: $O(kd)$, the time to form `Ztest`. There's also the dot product which takes $O(k)$ but that's negligible.\n",
    "- Approach 2: $O(kd)$, same as above\n",
    "- Approach 3: $O(nd)$, need to run through the training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
