{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 20: linear classifiers: multi-class\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=x-TU-0PMm-Q&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=21)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Unravel by Shape / Shift\n",
    "2. Snowman by Sia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a4 due March 10 (Wed next week)\n",
    "- Office hours\n",
    "  - Often empty according to TAs\n",
    "  - A great way to get extra help\n",
    "  - It's fine to ask about material from a while ago if you're behind\n",
    "- Next class, Monday March 8:\n",
    "  - I am unable to attend class due to an immovable family commitment.\n",
    "  - Regular class will be replaced with a bonus lecture by Daniele Reda (TA) on introduction to reinforcement learning.\n",
    "  - Attendance is optional. \n",
    "- I forgot to put a4 in Gradescope (again), will do very soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- review\n",
    "- max margin\n",
    "- motivation: POS tagging\n",
    "- one-vs-rest\n",
    "- shape of decision boundaries\n",
    "- softmax function\n",
    "- softmax loss\n",
    "- Frobenius norm\n",
    "- Q&A \n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "1. If your dataset is linearly separable that means you can get zero training error with logistic regression.\n",
    "2. If your dataset is linearly separable that means you can get zero training loss with logistic regression.\n",
    "3. With a multi-class linear classifier, we have one intercept _per class_ (note: this may not have been mentioned in the video?).\n",
    "4. The one-vs-all approach involves solving one optimization problem of higher dimension; the softmax approach involves solving many optimization problems each of lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapted from 2016W2 final\n",
    "\n",
    "For each of the models below, how many parameters does the model have? Your answer may depend on $n$, $d$, and perhaps additional quantities (if so, you need to define them). \n",
    "\n",
    "- (a) Logistic regression for binary classification.\n",
    "\n",
    "- (b) Multi-class logistic regression using softmax.\n",
    "\n",
    "- (c) Multi-class logistic regression using one-vs-all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2016W1 final\n",
    "\n",
    "![](img/L20/predict.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** At some point we changed the shape of $W$ from $d\\times k$ to $k\\times d$. Luckily this change was made before the 2017W2 videos were filmed. However, this question below is from 2016W1 and uses the old convention. We would write this as\n",
    "\n",
    "$$W = \\begin{bmatrix}-1 &+2\\\\+2 &0 \\\\ 0 &+3 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add-on to the above\n",
    "\n",
    "Consider training a one-vs-rest logistic regression on the dataset above. For the first of the 3 classifiers, what would the training data look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2017W1 final\n",
    "\n",
    "What is the key difference between “one vs. all” logistic regression and training using the softmax loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
